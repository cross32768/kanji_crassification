{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.0.0\n",
      "torchvision version: 0.2.1\n",
      "Is GPU available: True\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('torchvision version:', torchvision.__version__)\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('Is GPU available:', use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if use_gpu else 'cpu')\n",
    "\n",
    "# batchsize\n",
    "batchsize = 64\n",
    "\n",
    "# seed setting (warning : cuDNN's randomness is remaining)\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "if use_gpu:\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory settings\n",
    "root_dir = '../../data/'\n",
    "# directory for training data images\n",
    "image_dir = root_dir + 'kkanji2_expansion_can_get_radical/'\n",
    "# directory for save logs and trained weights\n",
    "save_dir = root_dir + 'kkanji2_result/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "# load json for label\n",
    "with open(root_dir + 'kanjivg_radical/utf16_to_radical.json') as f:\n",
    "    utf16_to_radical = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of radical: 1300\n"
     ]
    }
   ],
   "source": [
    "# prepare dict for one-hot encoding\n",
    "radical_set = set()\n",
    "for value in utf16_to_radical.values():\n",
    "    for v in value:\n",
    "        radical_set.add(v)\n",
    "radical_list = sorted(list(radical_set))\n",
    "\n",
    "radical_dict = {}\n",
    "for index, radical in enumerate(radical_list):\n",
    "    radical_dict[radical] = index\n",
    "\n",
    "n_radical = len(radical_dict)\n",
    "print('the number of radical:', n_radical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset class for image loading and label setting\n",
    "class KanjiRadicalDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_name_list, utf16_to_radical, radical_dict, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_name_list = sorted(image_name_list)\n",
    "        \n",
    "        self.utf16_to_radical = utf16_to_radical\n",
    "        self.radical_dict = radical_dict\n",
    "        \n",
    "        self.n_radical = len(radical_dict)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_name_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_name_list[idx]        \n",
    "        image = Image.open(self.image_dir + image_name)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        label = torch.zeros(self.n_radical)\n",
    "        utf16_code = image_name[:4]\n",
    "        radical_list = self.utf16_to_radical[utf16_code]\n",
    "        for radical in radical_list:\n",
    "            label[radical_dict[radical]] = 1\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training data: 94127\n",
      "The number of validation data: 23532\n"
     ]
    }
   ],
   "source": [
    "# make dataset and train test split\n",
    "train_name_list, validation_name_list = train_test_split(os.listdir(image_dir), test_size = 0.2, random_state = seed)\n",
    "\n",
    "tf_train = transforms.Compose([transforms.RandomCrop(64, padding=8), transforms.ToTensor()])\n",
    "tf_validation = transforms.ToTensor()\n",
    "\n",
    "train_data = KanjiRadicalDataset(image_dir, train_name_list, utf16_to_radical, radical_dict, transform=tf_train)\n",
    "validation_data = KanjiRadicalDataset(image_dir, validation_name_list, utf16_to_radical, radical_dict, transform=tf_validation)\n",
    "\n",
    "print('The number of training data:', len(train_data))\n",
    "print('The number of validation data:', len(validation_data))\n",
    "\n",
    "# make DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=batchsize, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size=batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = set()\n",
    "# for vs in utf16_to_radical.values():\n",
    "#     for v in vs:\n",
    "#        a.add(v)\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのラベルの分布を見る\n",
    "# radical_count = torch.zeros(n_radical)\n",
    "# for index, (image, label) in enumerate(validation_loader):\n",
    "#    radical_count += torch.sum(label, dim=0)\n",
    "#    print('\\rprogress[%d/%d]' % (index+1, len(validation_loader)), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# radical_count_non_zero = radical_count[radical_count > 0]\n",
    "# plt.xlim([0,500])\n",
    "# plt.ylim([0,100])\n",
    "# plt.hist(radical_count_non_zero, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar(range(len(radical_count_non_zero)), radical_count_non_zero)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/kuangliu/pytorch-cifar/blob/master/models/preact_resnet.pyより PreActResNet-18\n",
    "# 一番最初の入力チャネルを1チャネルに変更、フィルタ数を全体的に増やしてある\n",
    "# 本当は多分初期化をちゃんとやったほうが良いが、取り敢えずはこのまま\n",
    "# preactResNet-18の採用理由はkmnistの提案論文のbaselineに合わせるためだが、あれは32x32のkmnist, k49で使われたものなので、\n",
    "# 64x64のkkanjiに適用するのは微妙かも\n",
    "# データセットの提案論文ではmanifold mixupをdata augumentationとして採用していますが、取り敢えずはまだやっていない\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "    '''Pre-activation version of the BasicBlock.'''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "    \n",
    "class PreActResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(PreActResNet, self).__init__()\n",
    "        planes = 256\n",
    "        self.in_planes = planes\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_layer(block,   planes, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 2*planes, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 4*planes, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 8*planes, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(8*planes*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "# Todo:情報量ボトルネックがavg_poolの辺りにあるので、ImageNetのPreActResNetとかを参考に改善すること\n",
    "# これで良いっぽい？\n",
    "def PreActResNet18(num_classes):\n",
    "    return PreActResNet(PreActBlock, [2,2,2,2], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trainable parameters: 181213204\n",
      "\n",
      "Model:\n",
      " PreActResNet(\n",
      "  (conv1): Conv2d(1, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): PreActBlock(\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (1): PreActBlock(\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): PreActBlock(\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): PreActBlock(\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): PreActBlock(\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): PreActBlock(\n",
      "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): PreActBlock(\n",
      "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): PreActBlock(\n",
      "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=2048, out_features=1300, bias=True)\n",
      ")\n",
      "\n",
      "Loss function:\n",
      " BCEWithLogitsLoss()\n",
      "\n",
      "Optimizer:\n",
      " SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.1\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0.0005\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = PreActResNet18(n_radical)\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "\n",
    "print('The number of trainable parameters:', num_trainable_params)\n",
    "print('\\nModel:\\n', net)\n",
    "print('\\nLoss function:\\n', criterion)\n",
    "print('\\nOptimizer:\\n', optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_per_kanji_and_radical(outputs, labels):\n",
    "    with torch.no_grad():\n",
    "        outputs = torch.sigmoid(outputs) > 0.5\n",
    "        labels = labels.type(torch.uint8)\n",
    "        is_correct = (outputs == labels)\n",
    "        accuracy_per_kanji = (torch.sum(is_correct, dim=1) == outputs.size(1)).float().mean()\n",
    "        accuracy_per_radical = is_correct.float().mean()\n",
    "    return [accuracy_per_kanji, arrucary_per_radical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    net.train()\n",
    "    running_loss = 0\n",
    "    running_accuracy_per_kanji = 0\n",
    "    running_accuracy_per_radical = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        accuracies = calculate_accuracy_per_kanji_and_radical(outputs.detach(), labels)\n",
    "        running_accuracy_per_kanji += accuracies[0]\n",
    "        running_accuracy_per_radical += accuracies[1]\n",
    "        \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    accuracy_per_kanji = running_accuracy_per_kanji / len(train_loader)\n",
    "    accuracy_per_radical = running_accuracy_per_radical / len(train_loader)\n",
    "    \n",
    "    return train_loss, accuracy_per_kanji, accuracy_per_radical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(validation_loader):\n",
    "    net.eval()\n",
    "    running_loss = 0\n",
    "    running_accuracy_per_kanji = 0\n",
    "    running_accuracy_per_radical = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            accuracies = calculate_accuracy_per_kanji_and_radical(outputs.detach(), labels)\n",
    "            running_accuracy_per_kanji += accuracies[0]\n",
    "            running_accuracy_per_radical += accuracies[1]            \n",
    "\n",
    "    validation_loss = running_loss / len(validation_loader)\n",
    "    accuracy_per_kanji = running_accuracy_per_kanji / len(validation_loader)\n",
    "    accuracy_per_radical = running_accuracy_per_radical / len(validation_loader)\n",
    "    \n",
    "    return validation_loss, accuracy_per_kanji, accuracy_per_radical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "train_accuracy_per_kanji_list = []\n",
    "train_accuracy_per_radical_list = []\n",
    "\n",
    "validation_loss_list = []\n",
    "validation_accuracy_per_kanji_list = []\n",
    "validation_accuracy_per_radical_list = []\n",
    "\n",
    "n_epochs = 2\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_accuracy_per_kanji, train_accuracy_per_radical = train(train_loader)\n",
    "    validation_loss, validation_accuracy_per_kanji, validation_accuracy_per_radical = validation(validation_loader)\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accuracy_per_kanji_list.append(train_accuracy_per_kanji)\n",
    "    train_accuracy_per_radical_list.append(train_accuracy_per_radical)\n",
    "    \n",
    "    validation_loss_list.append(validation_loss)\n",
    "    validation_accuracy_per_kanji_list.append(validation_acccuracy_per_kanji)\n",
    "    validation_accuracy_per_radical_list.append(validation_arrucary_per_radical)\n",
    "    \n",
    "    print('epoch[%3d/%3d] train[loss:%1.4f accuracy_per_kanji:%1.4f accuracy_per_radical:%1.4f]' \\\n",
    "          % (epoch, n_epochs, train_loss, train_accuracy_per_kanji, train_accuracy_per_radical), \\\n",
    "          '\\n          validation[loss:%1.4f accuracy_per_kanji:%1.4f accuracy_per_radical:%1.4f]'\n",
    "          % (validation_loss, validation_accuracy_per_kanji, validation_accuracy_per_radical))\n",
    "    \n",
    "np.save(save_dir + 'train_loss_list.npy', np.array(train_loss_list))\n",
    "np.save(save_dir + 'train_accuracy_per_kanji_list.npy', np.array(train_accuracy_per_kanji_list))\n",
    "np.save(save_dir + 'train_accuracy_per_radical_list.npy', np.array(train_accuracy_per_radical_list))\n",
    "\n",
    "np.save(save_dir + 'validation_loss_list.npy', np.array(validation_loss_list))\n",
    "np.save(save_dir + 'validation_accuracy_per_kanji_list.npy', np.array(validation_accuracy_per_kanji_list))\n",
    "np.save(save_dir + 'validation_accuracy_per_radical_list.npy', np.array(validation_accuracy_per_radical_list))\n",
    "\n",
    "torch.save(net.state_dict(), save_dir + 'weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
